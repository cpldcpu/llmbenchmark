# LLM Benchmark Suite

A collection of benchmark experiments for evaluating different aspects of Large Language Models (LLMs).

## Projects

### [Raytracer](raytracer/Readme.md)
Evaluates LLMs' code generation capabilities by prompting them to create Python raytracers. Includes visual comparisons and consistency tests across multiple models.

### [Thinking Traces Analyzer](thinkingtraces/README.md)
Analyzes the reasoning patterns and chain-of-thought processes of various LLMs by examining statistical patterns in their thinking traces.

## License

This project is licensed under [CC0 1.0 Universal](LICENSE).

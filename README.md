# LLM Benchmark Experiments

A collection of benchmark experiments for evaluating different aspects of Large Language Models (LLMs).

## Projects

### [Raytracer](raytracer/Readme.md)
Evaluates code generation capabilities by prompting to create Python raytracers. Includes visual comparisons and consistency tests across multiple models.

### [Thinking Traces Analyzer](thinkingtraces/README.md)
Analyzes the reasoning patterns and chain-of-thought processes of various LLMs by examining statistical patterns in their thinking traces.

### [Fingerprinting](fingerprinting/README.md)
Attempt to tests LLM consistency and uniqueness across various topic domains by analyzing generation statistics for prompts with a single word response. Helps identify model-specific response patterns that can serve as "fingerprints" for different LLMs.

## License

This project is licensed under [CC0 1.0 Universal](LICENSE).
